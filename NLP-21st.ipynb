{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load dataset (assuming it's downloaded and named 'bbc-news.csv')\n",
        "df = pd.read_csv(\"/content/sample_data/bbc_news.csv\")\n",
        "\n",
        "# Keep only 'description' column\n",
        "texts = df['description'].dropna().astype(str)\n",
        "\n",
        "# Basic text cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation and numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
        "    return text\n",
        "\n",
        "df['clean_description'] = texts.apply(clean_text)\n",
        "print(df['clean_description'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkMyYZMNUcsv",
        "outputId": "a5ccc81f-1c40-43cf-d8b0-d53a728eaa26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    the ukrainian president says the country will ...\n",
            "1    jeremy bowen was on the frontline in irpin as ...\n",
            "2    one of the worlds biggest fertiliser firms say...\n",
            "3    the parents of the manchester arena bombings y...\n",
            "4    consumers are feeling the impact of higher ene...\n",
            "Name: clean_description, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "def tokenize_nltk(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
        "    return tokens\n",
        "\n",
        "df['tokens_nltk'] = df['clean_description'].apply(tokenize_nltk)\n",
        "\n",
        "# Flatten token list\n",
        "all_tokens = [token for tokens in df['tokens_nltk'] for token in tokens]\n",
        "\n",
        "# Top 10 frequent tokens\n",
        "freq_dist = Counter(all_tokens)\n",
        "print(\"Top 10 frequent tokens (NLTK):\")\n",
        "print(freq_dist.most_common(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9SNjG00Udz3",
        "outputId": "6e1005d9-6595-42cd-f9f6-c6295f1e833b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 frequent tokens (NLTK):\n",
            "[('says', 4561), ('world', 2030), ('bbc', 2011), ('people', 1989), ('england', 1922), ('first', 1905), ('new', 1894), ('say', 1676), ('cup', 1486), ('two', 1392)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load the small English language model\n",
        "# Run 'python -m spacy download en_core_web_sm' in your terminal if you don't have it\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample text data (e.g., from a 'description' column of a dataset)\n",
        "text_data = \"\"\"\n",
        "    Running is a great exercise. Many runners run daily.\n",
        "    Studies have shown that studying improves knowledge.\n",
        "    The best teams are not always the team with the best players.\n",
        "    This new policy was implemented. The implementation was successful.\n",
        "\"\"\"\n",
        "\n",
        "# Process the text with the spaCy pipeline\n",
        "doc = nlp(text_data)\n",
        "\n",
        "# Extract lemmas, excluding stop words and punctuation\n",
        "# Lemmatization reduces words like 'running' and 'ran' to their base form 'run'\n",
        "lemmas = [\n",
        "    token.lemma_.lower()\n",
        "    for token in doc\n",
        "    if not token.is_stop and not token.is_punct and not token.is_space\n",
        "]\n",
        "\n",
        "# Count the frequency of each lemma\n",
        "lemma_counts = Counter(lemmas)\n",
        "\n",
        "# Display the top 10 most frequent lemmas\n",
        "print(\"Top 10 Frequent Lemmas:\")\n",
        "for lemma, count in lemma_counts.most_common(10):\n",
        "    print(f\"{lemma}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBNCF_n6Uml9",
        "outputId": "46745048-f85c-4b1d-92c8-4a2c50f1b058"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Frequent Lemmas:\n",
            "run: 2\n",
            "good: 2\n",
            "team: 2\n",
            "great: 1\n",
            "exercise: 1\n",
            "runner: 1\n",
            "daily: 1\n",
            "study: 1\n",
            "show: 1\n",
            "studying: 1\n"
          ]
        }
      ]
    }
  ]
}